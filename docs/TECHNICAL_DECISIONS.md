# üèóÔ∏è Technical Decision Records (TDRs)

## Overview
This document tracks important technical decisions made during the development of the Temporal Optimization system. Each decision includes context, alternatives considered, and rationale.

---

## TDR-001: Temporal Optimization Architecture - "Prepped Kitchen" Model

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Implement decoupled data ingestion and serving architecture

### Context
Original system was "cooking on-demand" - making real-time API calls for every user request, causing:
- 5-30 second response times
- High API costs ($0.05-0.20 per request)
- Rate limiting issues
- Unpredictable performance

### Decision
Implement "Prepped Kitchen" architecture with three phases:
1. **Phase 1 (Collector)**: Background news collection via cron jobs
2. **Phase 2 (Intelligence)**: Offline AI processing and enrichment
3. **Phase 3 (Serving)**: Sub-millisecond queries from pre-processed data

### Alternatives Considered
1. **Status Quo**: Keep real-time API calls
2. **Caching Layer**: Add Redis caching to existing system
3. **Hybrid Approach**: Cache some data, real-time for others

### Rationale
- **Performance**: Sub-1ms response times achieved (99.97% improvement)
- **Cost**: 99% reduction in per-request costs
- **Reliability**: No user-facing API rate limits
- **Scalability**: Can serve thousands from pre-processed data

### Results
‚úÖ **Response Time**: <1ms (vs 5-30s before)  
‚úÖ **Cost**: <$0.001 per request (vs $0.05-0.20)  
‚úÖ **Reliability**: No API rate limits during serving  
‚úÖ **User Experience**: Consistent, predictable performance  

---

## TDR-002: Database Choice - SQLite for Raw Data Storage

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Use SQLite for raw article storage and initial processing

### Context
Needed reliable, fast database for storing raw news articles and metadata.

### Decision
SQLite with JSON columns for flexible metadata storage:
```sql
CREATE TABLE raw_articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source TEXT NOT NULL,
    crypto_symbol TEXT NOT NULL,
    title TEXT NOT NULL,
    content TEXT,
    url TEXT UNIQUE NOT NULL,
    raw_data TEXT,  -- JSON for flexible metadata
    processed BOOLEAN DEFAULT FALSE
);
```

### Alternatives Considered
1. **PostgreSQL**: Full RDBMS with JSON support
2. **MongoDB**: Document database
3. **Redis**: In-memory key-value store
4. **File-based storage**: JSON files

### Rationale
- **Simplicity**: No external dependencies, works on Replit
- **Performance**: Fast enough for our scale (<100k articles)
- **JSON Support**: Flexible metadata storage
- **ACID Compliance**: Data integrity guarantees
- **Zero Configuration**: No setup required

### Trade-offs
- **Scaling Limits**: Single file, not distributed
- **Concurrent Writes**: Limited compared to PostgreSQL
- **Advanced Features**: No full-text search, limited analytics

### Migration Path
When scaling beyond SQLite limits:
1. **Phase 1**: PostgreSQL for structured data
2. **Phase 2**: Add Redis for caching
3. **Phase 3**: Consider time-series DB for analytics

---

## TDR-003: API Integration Strategy - Direct HTTP Calls vs SDKs

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Use direct HTTP calls with httpx instead of heavy SDK libraries

### Context
Need to integrate with NewsAPI, Tavily, OpenAI, and other external APIs.

### Decision
Use `httpx` for direct HTTP calls with custom client classes:
```python
async def fetch_news_articles(terms: List[str]) -> List[Dict]:
    async with httpx.AsyncClient() as client:
        resp = await client.get(NEWSAPI_URL, params=params)
        return resp.json()
```

### Alternatives Considered
1. **Official SDKs**: newsapi-python, openai-python, etc.
2. **Requests Library**: Synchronous HTTP calls
3. **Aiohttp**: Alternative async HTTP client

### Rationale
- **App Size**: Reduces dependencies and deployment size
- **Control**: Full control over request/response handling
- **Async Support**: Native async/await support
- **Flexibility**: Easy to customize headers, retries, timeouts
- **Debugging**: Easier to debug HTTP issues

### Trade-offs
- **More Code**: Need to implement client logic ourselves
- **Maintenance**: Need to handle API changes manually
- **Type Safety**: Less type safety than official SDKs

---

## TDR-004: Error Handling Strategy - Graceful Degradation

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Implement graceful degradation with fallbacks for missing components

### Context
System needs to work in various environments (local, Replit, production) with different dependency availability.

### Decision
Graceful fallback pattern throughout the system:
```python
try:
    from utils.milvus import EnhancedVectorRAG
    vector_rag = EnhancedVectorRAG()
except ImportError:
    vector_rag = None
    logger.warning("Vector RAG not available")

# Later in code
if vector_rag:
    await vector_rag.store_embeddings(article)
else:
    logger.info("Skipping vector storage - not available")
```

### Alternatives Considered
1. **Fail Fast**: Crash if dependencies missing
2. **Required Dependencies**: Force all components to be available
3. **Mock Objects**: Create mock implementations

### Rationale
- **Deployment Flexibility**: Works in constrained environments
- **Development Speed**: Can develop without all services running
- **Reliability**: System continues working with partial failures
- **User Experience**: Degraded functionality vs no functionality

### Implementation Pattern
1. **Import Level**: Try/except on imports
2. **Initialization Level**: Check if objects are None
3. **Execution Level**: Skip operations gracefully
4. **Logging**: Always log what's happening

---

## TDR-005: Collection Scheduling - Cron Jobs vs Background Tasks

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Use external cron jobs for collection scheduling

### Context
Need to run news collection and processing on a schedule without blocking the web application.

### Decision
External cron job approach:
```bash
# Cron job runs this
python3 run_temporal_cycle.py --mode collect --hours-back 168
```

### Alternatives Considered
1. **FastAPI Background Tasks**: Built-in background processing
2. **Celery**: Distributed task queue
3. **APScheduler**: Python job scheduling library
4. **External Cron**: System-level cron jobs

### Rationale
- **Separation of Concerns**: Web app serves, cron collects
- **Reliability**: If web app crashes, collection continues
- **Resource Management**: Collection doesn't impact API performance
- **Simplicity**: No additional infrastructure needed
- **Replit Compatible**: Works with Replit's cron system

### Trade-offs
- **Coordination**: Need to coordinate between processes
- **Monitoring**: Harder to monitor from web interface
- **State Management**: Need shared state via database

---

## TDR-006: Router Architecture - Monolithic vs Modular

**Date**: 2025-08-08  
**Status**: üîÑ In Progress  
**Decision**: Transition from monolithic main.py to modular router architecture

### Context
Current implementation has all endpoints in main.py (2000+ lines), making it hard to maintain and test.

### Decision
Implement clean router separation:
```python
# main.py - clean and focused
app.include_router(temporal_router.router, prefix="/api/temporal")

# routers/temporal_router.py - domain-specific
@router.get("/optimized-news")
@router.post("/trigger-coin-selection")
```

### Alternatives Considered
1. **Keep Monolithic**: All code in main.py
2. **Microservices**: Separate applications
3. **Modular Monolith**: Single app, multiple routers

### Rationale
- **Maintainability**: Easier to find and modify code
- **Testing**: Can test routers independently
- **Team Development**: Multiple developers can work on different routers
- **Code Organization**: Clear separation of concerns
- **FastAPI Best Practice**: Recommended pattern

### Implementation Plan
1. **Phase 1**: Create temporal_router.py
2. **Phase 2**: Move endpoints from main.py
3. **Phase 3**: Add proper models and services
4. **Phase 4**: Repeat for other domains

---

## TDR-007: Dynamic Coin Selection Strategy

**Date**: 2025-08-08  
**Status**: üìã Planned  
**Decision**: Implement multi-source dynamic coin selection system

### Context
Current system uses hardcoded list of 8 crypto symbols. Need intelligent, dynamic selection based on market conditions.

### Decision
Multi-source selection algorithm:
```python
coins = []
coins.extend(get_trending_coins())      # Market data
coins.extend(get_portfolio_coins())     # User holdings
coins.extend(get_social_trending())     # Social sentiment
return apply_filters_and_rank(coins)
```

### Data Sources
1. **Market Data**: LiveCoinWatch, CoinGecko (price/volume changes)
2. **Portfolio Data**: User holdings from database
3. **Social Data**: Twitter/Reddit mentions and sentiment
4. **Technical Data**: Trading volume, market cap changes

### Selection Criteria
- **Trending**: Price change >5%, volume surge >200%
- **Portfolio**: User holdings >$100, recent additions
- **Social**: Mention frequency >threshold, positive sentiment
- **Filters**: Exclude stablecoins, minimum market cap, maximum risk

### Benefits
- **Relevance**: Always analyzing most relevant coins
- **Personalization**: Adapts to user portfolios
- **Market Awareness**: Responds to market conditions
- **Cost Efficiency**: Focuses resources on important coins

---

## TDR-008: External Integration Architecture - Webhooks vs Polling

**Date**: 2025-08-08  
**Status**: üìã Planned  
**Decision**: Implement webhook-based integration with N8N and external systems

### Context
Need to integrate with external workflow systems (N8N) and respond to market events in real-time.

### Decision
Webhook-based push architecture:
```python
@router.post("/trigger-coin-selection")
async def webhook_trigger(request: TriggerRequest):
    # External system pushes data to us
    # We respond immediately with background processing
```

### Alternatives Considered
1. **Polling**: Regularly check external systems
2. **WebSockets**: Real-time bidirectional communication
3. **Message Queues**: RabbitMQ, Redis pub/sub

### Rationale
- **Real-time Response**: Immediate reaction to market events
- **Efficiency**: No unnecessary polling
- **N8N Integration**: Native webhook support
- **Scalability**: Can handle multiple external sources
- **Simplicity**: HTTP-based, easy to implement and debug

### Integration Examples
- **Market Alerts**: Price spike ‚Üí N8N ‚Üí Webhook ‚Üí Coin selection
- **Social Alerts**: Tweet surge ‚Üí N8N ‚Üí Webhook ‚Üí News collection
- **Portfolio Events**: New holding ‚Üí N8N ‚Üí Webhook ‚Üí Analysis update

---

## TDR-009: Performance Optimization Strategy

**Date**: 2025-08-08  
**Status**: ‚úÖ Implemented  
**Decision**: Optimize for sub-millisecond response times through pre-processing

### Context
Target: <1ms response times for news queries to provide exceptional user experience.

### Decision
Complete pre-processing approach:
1. **All expensive operations offline**: API calls, AI processing, embeddings
2. **Simple database queries**: SELECT with basic filtering
3. **Minimal computation**: Pre-calculated insights and scores
4. **Optimized queries**: Proper indexing, LIMIT clauses

### Results Achieved
- **Query Time**: 0.5-0.6ms consistently
- **Database Size**: Efficient storage with JSON metadata
- **Memory Usage**: Low memory footprint
- **CPU Usage**: Minimal processing during serving

### Future Optimizations
- **Indexing**: Add database indexes for common queries
- **Caching**: Redis layer for frequently accessed data
- **Connection Pooling**: Reuse database connections
- **Query Optimization**: Analyze slow queries

---

## TDR-010: Data Quality and Validation Strategy

**Date**: 2025-08-08  
**Status**: üîÑ In Progress  
**Decision**: Implement comprehensive data quality checks throughout pipeline

### Context
Need to ensure high-quality, relevant news data and prevent garbage in/garbage out scenarios.

### Decision
Multi-layer validation approach:
1. **Collection Layer**: Source reliability, content quality
2. **Processing Layer**: AI confidence scores, entity validation
3. **Serving Layer**: Relevance scoring, recency checks
4. **Monitoring Layer**: Quality metrics, anomaly detection

### Quality Criteria
- **Source Reliability**: Track success rates by news source
- **Content Quality**: Minimum length, language detection, spam filtering
- **Relevance**: Crypto keyword presence, entity extraction confidence
- **Freshness**: Publication date validation, duplicate detection
- **AI Confidence**: Sentiment analysis confidence scores

### Implementation
```python
class DataQualityFilter:
    def validate_article(self, article: Dict) -> bool:
        return (
            self.has_crypto_relevance(article) and
            self.meets_quality_standards(article) and
            self.is_from_reliable_source(article) and
            not self.is_duplicate(article)
        )
```

---

## üìã Future Decisions Needed

### High Priority
1. **TDR-011**: Production Database Strategy (SQLite ‚Üí PostgreSQL migration)
2. **TDR-012**: Caching Layer Architecture (Redis integration)
3. **TDR-013**: Authentication and Authorization Strategy
4. **TDR-014**: Error Monitoring and Alerting System

### Medium Priority
1. **TDR-015**: Horizontal Scaling Strategy
2. **TDR-016**: Real-time Data Streaming Architecture
3. **TDR-017**: Machine Learning Pipeline Integration
4. **TDR-018**: API Versioning and Backward Compatibility

### Low Priority
1. **TDR-019**: Multi-tenant Architecture
2. **TDR-020**: International Market Support
3. **TDR-021**: Mobile API Optimization
4. **TDR-022**: Advanced Analytics and Reporting

---

## üìä Decision Impact Matrix

| Decision | Performance Impact | Complexity | Maintenance | Cost |
|----------|-------------------|------------|-------------|------|
| TDR-001 (Prepped Kitchen) | üü¢ Very High | üü° Medium | üü¢ Low | üü¢ Very Low |
| TDR-002 (SQLite) | üü¢ High | üü¢ Very Low | üü¢ Very Low | üü¢ Zero |
| TDR-003 (Direct HTTP) | üü° Medium | üü° Medium | üü° Medium | üü¢ Low |
| TDR-004 (Graceful Degradation) | üü° Low | üü° Medium | üü¢ Low | üü¢ Zero |
| TDR-005 (Cron Jobs) | üü¢ High | üü¢ Low | üü° Medium | üü¢ Low |

**Legend**: üü¢ Positive | üü° Neutral | üî¥ Negative

---

*Last Updated: 2025-08-08*  
*Next Review: When making significant architectural changes*
